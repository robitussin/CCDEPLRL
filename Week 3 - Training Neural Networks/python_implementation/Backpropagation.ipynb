{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/neural-networks-forward-pass-and-backpropagation-be3b75a1cfcc"
      ],
      "metadata": {
        "id": "49LbghVYoYPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "kgN2DDkKQjjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for repeatability\n",
        "\n",
        "np.random.seed(68)\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "torch.set_default_tensor_type(torch.FloatTensor)"
      ],
      "metadata": {
        "id": "1cegwr0VQvTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input to the neural network\n",
        "t_c = [1.0]\n",
        "t_u = [1.0]\n",
        "t_c = np.array(t_c)\n",
        "t_u = np.array(t_u)"
      ],
      "metadata": {
        "id": "C17umJLoRMKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numpy_to_variable(r):\n",
        "    return Variable(torch.from_numpy(r).float(), requires_grad=True).to(device)"
      ],
      "metadata": {
        "id": "oM9KhvFlRPGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network with two hidden layers\n",
        "\n",
        "<img src= \"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0RP2z_jewe_IPaJhAxPAwQ.png\" />"
      ],
      "metadata": {
        "id": "ZLF1HUYCo9-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network with computation of variables\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*kNOa3-rGup6t_qspu9ZJtA.png\" />"
      ],
      "metadata": {
        "id": "ZSjsFLntoymu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the netowrk configuration\n",
        "\n",
        "model_nn = nn.Sequential(\n",
        "    nn.Linear(1,2),  # input layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(2,2),  # hidden layer\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(2,1)   # output layer\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "0o7oVql4RQoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Do 2 epochs to check updated weights and biases\n",
        "n_epochs = 2\n",
        "learning_rate = 1e-2"
      ],
      "metadata": {
        "id": "KlKB51U1RmFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optL = optim.SGD(model_nn.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "83T6M6lbRnFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.MSELoss(size_average=False,reduce=False,reduction='sum')"
      ],
      "metadata": {
        "id": "jnwepCrKRoCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1_u = numpy_to_variable(t_u[:])\n",
        "t_ucol = torch.tensor(t1_u, device=device, requires_grad=True)\n",
        "t1_c = numpy_to_variable(t_c[:])\n",
        "t_ccol = torch.tensor(t1_c, device=device, requires_grad=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awP3rtVCRzpi",
        "outputId": "ed9483e4-d6cd-45e2-efa2-71efaba73a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-71-209491f2b69d>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t_ucol = torch.tensor(t1_u, device=device, requires_grad=True)\n",
            "<ipython-input-71-209491f2b69d>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t_ccol = torch.tensor(t1_c, device=device, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivate of loss with respect to y-hat\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:832/format:webp/1*XJVeMfJxSLrZz-MisvkM9Q.png\" />"
      ],
      "metadata": {
        "id": "7Ztk2pbepmdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivate of loss with respect to $w_{7}$\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:802/format:webp/1*5pIm541OowoQAmIimMcFPw.png\" />\n",
        "\n"
      ],
      "metadata": {
        "id": "cPWoUPsapq0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivate of loss with respect to $w_{8}$ and Derivate of loss with respect to $b_{5}$\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3n7b9AjtcqkmNFRhfX0uQ.png\" />"
      ],
      "metadata": {
        "id": "kl1V9plGp3BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivative of loss with respect to $w_{3}$\n",
        "\n",
        "Derivative of loss with respect to $w_{5}$\n",
        "\n",
        "Derivative of loss with respect to $b_{3}$"
      ],
      "metadata": {
        "id": "B0oevFvjrdKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src= \"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*3iqfemgszZkFDpRmNzXH0w.png\"/>"
      ],
      "metadata": {
        "id": "0Vb4AXFCp7jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivative of loss with respect to $w_{6}$\n",
        "\n",
        "Derivative of loss with respect to $w_{4}$\n",
        "\n",
        "Derivative of loss with respect to $b_{4}$"
      ],
      "metadata": {
        "id": "kWSNPEGhr07v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*02kfP92RRNvlQ0bhH4uFGg.png\" />"
      ],
      "metadata": {
        "id": "F33KZKzZqGm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivative of y-hat with respect to $a_{1}$\n",
        "\n",
        "Derivative of loss with respect to $w_{1}$\n",
        "\n",
        "Derivative of loss with respect to $b_{1}$"
      ],
      "metadata": {
        "id": "HXJJx92fr59A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*8WqQP3MPKe21QC44tCLwGg.png\" />"
      ],
      "metadata": {
        "id": "TfOuKqMkqMP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Derivative of y-hat with respect to $a_{2}$\n",
        "\n",
        "Derivative of loss with respect to $w_{2}$\n",
        "\n",
        "Derivative of loss with respect to $b_{2}$"
      ],
      "metadata": {
        "id": "WK42o7mLsDT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*GWmvr220wUaUGFZ-QD1X4w.png\" />"
      ],
      "metadata": {
        "id": "oRq7KyUUqTzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g4XWpGmPvqZ",
        "outputId": "44efd32c-321d-4c3c-bd99-1330d26cab1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0\n",
            "weight_input_layer:  [[-0.00748682]\n",
            " [ 0.5364436 ]]\n",
            " \n",
            "weight_hidden_layer:  [[-0.27234524  0.18961589]\n",
            " [-0.01401003  0.5606575 ]]\n",
            " \n",
            "weight_output_layer:  [[-0.21369691 -0.13899271]]\n",
            " \n",
            "output: tensor([[-0.7015]], grad_fn=<AddmmBackward0>)\n",
            "loss: tensor([[2.8952]], grad_fn=<MseLossBackward0>)\n",
            " \n",
            "grad_w_input_layer:  [[0.]\n",
            " [0.]]\n",
            " \n",
            "grad_w_hidden_layer:  [[0. 0.]\n",
            " [0. 0.]]\n",
            " \n",
            "gradw_output_layer:  [[ 0.         -0.63674814]]\n",
            " \n",
            "learning rate: 0.01\n",
            "--------------------------------\n",
            "epoch:1\n",
            "weight_input_layer:  [[-0.00748682]\n",
            " [ 0.5364436 ]]\n",
            " \n",
            "weight_hidden_layer:  [[-0.27234524  0.18961589]\n",
            " [-0.01401003  0.5606575 ]]\n",
            " \n",
            "weight_output_layer:  [[-0.21369691 -0.13262524]]\n",
            " \n",
            "output: tensor([[-0.6657]], grad_fn=<AddmmBackward0>)\n",
            "loss: tensor([[2.7745]], grad_fn=<MseLossBackward0>)\n",
            " \n",
            "grad_w_input_layer:  [[0.]\n",
            " [0.]]\n",
            " \n",
            "grad_w_hidden_layer:  [[0. 0.]\n",
            " [0. 0.]]\n",
            " \n",
            "gradw_output_layer:  [[ 0.       -0.607575]]\n",
            " \n",
            "learning rate: 0.01\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "for epochs in range(n_epochs):\n",
        "\n",
        "    print(f\"epoch:{epochs}\")\n",
        "    t_u1 = torch.cat([t_ucol.view((-1, 1))], dim=1)\n",
        "    t_c1 = torch.cat([t_ccol.view((-1, 1))], dim=1)\n",
        "\n",
        "    # Extract the random weights and biases from the model\n",
        "    wt_L0 = model_nn[0].weight.detach().to('cpu').numpy()\n",
        "    # bias_L0 = model_nn[0].bias.detach().to('cpu').numpy()\n",
        "\n",
        "    wt_L2 = model_nn[2].weight.detach().to('cpu').numpy()\n",
        "    # bias_L2 = model_nn[2].bias.detach().to('cpu').numpy()\n",
        "\n",
        "    wt_L4 = model_nn[4].weight.detach().to('cpu').numpy()\n",
        "    # bias_L4 = model_nn[4].bias.detach().to('cpu').numpy()\n",
        "\n",
        "\n",
        "    print(\"weight_input_layer: \", wt_L0)\n",
        "    # print(f\"epoch:{epochs}, bias_input_layer: {bias_L0}\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"weight_hidden_layer: \", wt_L2)\n",
        "    # print(f\"epoch:{epochs}, bias_hidden_layer: {bias_L2}\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"weight_output_layer: \", wt_L4)\n",
        "    # print(f\"epoch:{epochs}, bias_output_layer: {bias_L4}\")\n",
        "    print(\" \")\n",
        "\n",
        "#  print output from forward feed\n",
        "    t_output = model_nn(t_u1)\n",
        "    print(\"output:\",t_output)\n",
        "\n",
        "    loss_train = loss_fn(t_output,t_c1)\n",
        "    print(\"loss:\",loss_train)\n",
        "    print(\" \")\n",
        "\n",
        "#  zero out gradients and compute gradients of loss with respect to\n",
        "#  weights and biases.\n",
        "\n",
        "    optL.zero_grad()\n",
        "    loss_train.backward(retain_graph=True)\n",
        "\n",
        "    gradw_L0 = model_nn[0].weight.grad.detach().to('cpu').numpy()\n",
        "    # gradb_L0 = model_nn[0].bias.grad.detach().to('cpu').numpy()\n",
        "\n",
        "    gradw_L2 = model_nn[2].weight.grad.detach().to('cpu').numpy()\n",
        "    # gradb_L2 = model_nn[2].bias.grad.detach().to('cpu').numpy()\n",
        "\n",
        "    gradw_L4 = model_nn[4].weight.grad.detach().to('cpu').numpy()\n",
        "    # gradb_L4 = model_nn[4].bias.grad.detach().to('cpu').numpy()\n",
        "\n",
        "    print(\"grad_w_input_layer: \", gradw_L0)\n",
        "    # print(f\"epoch:{epochs}, grad_b_input_layer: {gradb_L0}\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"grad_w_hidden_layer: \", gradw_L2)\n",
        "    # print(f\"epoch:{epochs}, grad_b_hidden_layer: {gradb_L2}\")\n",
        "    print(\" \")\n",
        "\n",
        "    print(\"gradw_output_layer: \", gradw_L4)\n",
        "    #print(f\"epoch:{epochs}, gradb_output_layer: {gradb_L4}\")\n",
        "    print(\" \")\n",
        "\n",
        "    curr_lr = optL.param_groups[0][\"lr\"]\n",
        "    print(\"learning rate:\",curr_lr)\n",
        "\n",
        "    optL.step()\n",
        "\n",
        "    print(\"--------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual calculation\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1njvMZzPPJWGygW54OFpX7eu740fCnYjqqdgujQtZaPM/edit#gid=1501293754"
      ],
      "metadata": {
        "id": "O7CgitX8m4rp"
      }
    }
  ]
}